{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup \n",
    "\n",
    "Please run the code below to mount drive if you are running on colab.\n",
    "\n",
    "Please ignore if you are running on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/MiniGPT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling and Transformers\n",
    "\n",
    "The project will consist of two broad parts. \n",
    "\n",
    "1. **Baseline Generative Language Model**: We will train a simple Bigram language model on the text data. We will use this model to generate a mini story. \n",
    "2. **Implementing Mini GPT**: We will implement a mini version of the GPT model layer by layer and attempt to train it on the text data. You will then load pretrained weights provided and generate a mini story. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some general instructions \n",
    "\n",
    "1. Please keep the name of layers consistent with what is requested in the `model.py` file for each layer, this helps us test in each function independently. \n",
    "2. Please check to see if the bias is to be set to false or true for all linear layers (it is mentioned in the doc string)\n",
    "3. As a general rule please read the docstring well, it contains information you will need to write the code. \n",
    "4. All configs are defined in `config.py` for the first part. While you are writing the code, do not change the values in the config file since we use them to test. Once you have passed all the tests please feel free to vary the parameter as you please.\n",
    "5. You will need to fill in `train.py` and run it to train the model. If you are running into memory issues please feel free to change the `batch_size` in the `config.py` file. If you are working on Colab please make sure to use the GPU runtime and feel free to copy over the training code to the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy torch tiktoken wandb einops # Install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BigramLanguageModel, SingleHeadAttention, MultiHeadAttention, FeedForwardLayer, LayerNorm, TransformerLayer, MiniGPT\n",
    "from config import BigramConfig, MiniGPTConfig\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not provided, download from https://drive.google.com/file/d/1g09qUM9WibdfQVgkj6IAj8K2S3SGwc91/view?usp=sharing\n",
    "path_to_bigram_tester = \"./pretrained_models/bigram_tester.pt\" # Load the bigram model with name bigram_tester.pt\n",
    "path_to_gpt_tester = \"./pretrained_models/minigpt_tester.pt\" # Load the gpt model with name minigpt_tester.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bigram Language Model (10 points)\n",
    "\n",
    "A bigram language model is a type of probabilistic language model that predicts a word given the previous word in the sequence. The model is trained on a text corpus and learns the probability of a word given the previous word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Bigram model (5 points)\n",
    "\n",
    "Please complete the `BigramLanguageModel` class in model.py. We will model a Bigram language model using a simple MLP with one hidden layer. The model will take in the previous word index and output the logits over the vocabulary for the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test implementation for Bigram Language Model\n",
    "model = BigramLanguageModel(BigramConfig)\n",
    "tests.check_bigram(model, path_to_bigram_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Bigram Language Model (2.5 points)\n",
    "\n",
    "Complete the code in `train.py` to train the Bigram language model on the text data. Please provide plots for both the training and validation in the cell below.\n",
    "\n",
    "Some notes on the training process:\n",
    "\n",
    "1. You should be able to train the model slowly on your local machine.\n",
    "2. Training it on Colab will help with speed.\n",
    "3.  <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You should see it saturate to a value close to around 5-6 but as long as you see it decreasing then saturating you should be good.\n",
    "4. Please log the loss curves either on wandb, tensorboard or any other logger of your choice and please attach them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: 3.27M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manwchatto\u001b[0m (\u001b[33manwchatto-ucla\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anwesha/239AS_2/Project_3/wandb/run-20250519_002300-lw4q4vzv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anwchatto-ucla/dl2_proj3/runs/lw4q4vzv' target=\"_blank\">swept-dust-56</a></strong> to <a href='https://wandb.ai/anwchatto-ucla/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anwchatto-ucla/dl2_proj3' target=\"_blank\">https://wandb.ai/anwchatto-ucla/dl2_proj3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anwchatto-ucla/dl2_proj3/runs/lw4q4vzv' target=\"_blank\">https://wandb.ai/anwchatto-ucla/dl2_proj3/runs/lw4q4vzv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Logging Inteval: 10000\n",
      "Train len 473591\n",
      "Batch size 32\n",
      "Evaluating Model 0\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  40989480.98461914\n",
      "Iteration 0, Train Loss: 10.824810981750488 Eval Loss: 10.81877464796152\n",
      "  batch 1000 loss: 0.004974021434783936\n",
      "  batch 2000 loss: 0.005717559814453125\n",
      "  batch 3000 loss: 0.00365205192565918\n",
      "  batch 4000 loss: 0.004598745822906494\n",
      "  batch 5000 loss: 0.005582768440246582\n",
      "  batch 6000 loss: 0.005040801048278808\n",
      "  batch 7000 loss: 0.0032807955741882325\n",
      "  batch 8000 loss: 0.005333069324493408\n",
      "  batch 9000 loss: 0.005066819667816162\n",
      "  batch 10000 loss: 0.004794302940368652\n",
      "Evaluating Model 10000\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  15245228.141799927\n",
      "Iteration 10000, Train Loss: 5.806027889251709 Eval Loss: 4.023829620696699\n",
      "  batch 11000 loss: 0.005302007675170898\n",
      "  batch 12000 loss: 0.004838509559631348\n",
      "  batch 13000 loss: 0.0049011225700378415\n",
      "  batch 14000 loss: 0.004231267929077148\n",
      "  batch 15000 loss: 0.004204058647155761\n",
      "  batch 16000 loss: 0.004908410549163819\n",
      "  batch 17000 loss: 0.0052473778724670414\n",
      "  batch 18000 loss: 0.005200817584991455\n",
      "  batch 19000 loss: 0.004652397155761719\n",
      "  batch 20000 loss: 0.00477155351638794\n",
      "Evaluating Model 20000\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  15129118.964828491\n",
      "Iteration 20000, Train Loss: 4.178015232086182 Eval Loss: 3.9931837332631495\n",
      "  batch 21000 loss: 0.0048084230422973636\n",
      "  batch 22000 loss: 0.004518750667572021\n",
      "  batch 23000 loss: 0.005138218879699707\n",
      "  batch 24000 loss: 0.004450829029083252\n",
      "  batch 25000 loss: 0.0037119176387786865\n",
      "  batch 26000 loss: 0.00479921293258667\n",
      "  batch 27000 loss: 0.005578598499298096\n",
      "  batch 28000 loss: 0.0034216210842132567\n",
      "  batch 29000 loss: 0.005135415554046631\n",
      "  batch 30000 loss: 0.004782097339630127\n",
      "Evaluating Model 30000\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  15132337.777671814\n",
      "No improvement in eval loss. Count = 1/3\n",
      "Iteration 30000, Train Loss: 3.9671826362609863 Eval Loss: 3.994033307591718\n",
      "  batch 31000 loss: 0.005120843887329102\n",
      "  batch 32000 loss: 0.0051157112121582035\n",
      "  batch 33000 loss: 0.004493396759033203\n",
      "  batch 34000 loss: 0.0055880208015441895\n",
      "  batch 35000 loss: 0.004342808723449707\n",
      "  batch 36000 loss: 0.0036774282455444337\n",
      "  batch 37000 loss: 0.005423797607421875\n",
      "  batch 38000 loss: 0.004264708518981934\n",
      "  batch 39000 loss: 0.004543434143066407\n",
      "  batch 40000 loss: 0.005020881175994873\n",
      "Evaluating Model 40000\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  15113965.429336548\n",
      "Iteration 40000, Train Loss: 5.481503963470459 Eval Loss: 3.989184105025145\n",
      "  batch 41000 loss: 0.004642388820648193\n",
      "  batch 42000 loss: 0.004750207424163818\n",
      "  batch 43000 loss: 0.003711050748825073\n",
      "  batch 44000 loss: 0.004045965194702149\n",
      "  batch 45000 loss: 0.004412364482879638\n",
      "  batch 46000 loss: 0.004403239250183106\n",
      "  batch 47000 loss: 0.004360567092895508\n",
      "  batch 48000 loss: 0.004445030212402344\n",
      "  batch 49000 loss: 0.004821699142456054\n",
      "  batch 50000 loss: 0.0059126391410827634\n",
      "Evaluating Model 50000\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  15083153.009307861\n",
      "Iteration 50000, Train Loss: 4.500168800354004 Eval Loss: 3.981051466586181\n",
      "  batch 51000 loss: 0.0056374711990356445\n",
      "  batch 52000 loss: 0.004646697998046875\n",
      "  batch 53000 loss: 0.0041788525581359865\n",
      "  batch 54000 loss: 0.004428491115570068\n",
      "  batch 55000 loss: 0.0034737329483032226\n",
      "  batch 56000 loss: 0.005499123096466064\n",
      "  batch 57000 loss: 0.0045111489295959475\n",
      "  batch 58000 loss: 0.006061489105224609\n",
      "  batch 59000 loss: 0.004696951866149902\n",
      "  batch 60000 loss: 0.005325529098510742\n",
      "Evaluating Model 60000\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  15070278.46314621\n",
      "Iteration 60000, Train Loss: 4.0782623291015625 Eval Loss: 3.9776533554056575\n",
      "  batch 61000 loss: 0.0040394287109375\n",
      "  batch 62000 loss: 0.0045044107437133786\n",
      "  batch 63000 loss: 0.005724696636199951\n",
      "  batch 64000 loss: 0.004643142700195313\n",
      "  batch 65000 loss: 0.004648535251617431\n",
      "  batch 66000 loss: 0.005108572006225586\n",
      "  batch 67000 loss: 0.004307381629943848\n",
      "  batch 68000 loss: 0.005702144145965577\n",
      "  batch 69000 loss: 0.004526485919952392\n",
      "  batch 70000 loss: 0.005118056297302246\n",
      "Evaluating Model 70000\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  15023009.413856506\n",
      "Iteration 70000, Train Loss: 4.587998867034912 Eval Loss: 3.9651771498084076\n",
      "  batch 71000 loss: 0.003749931335449219\n",
      "  batch 72000 loss: 0.005127687454223633\n",
      "  batch 73000 loss: 0.004973254680633545\n",
      "  batch 74000 loss: 0.005316810131072998\n",
      "  batch 75000 loss: 0.0046036171913146975\n",
      "  batch 76000 loss: 0.005134759426116943\n",
      "  batch 77000 loss: 0.004811874866485596\n",
      "  batch 78000 loss: 0.005363302230834961\n",
      "  batch 79000 loss: 0.004295323848724365\n",
      "  batch 80000 loss: 0.003946974992752075\n",
      "Evaluating Model 80000\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  15068272.796188354\n",
      "No improvement in eval loss. Count = 1/3\n",
      "Iteration 80000, Train Loss: 4.468671798706055 Eval Loss: 3.977123979128753\n",
      "  batch 81000 loss: 0.005607658863067627\n",
      "  batch 82000 loss: 0.004919509410858155\n",
      "  batch 83000 loss: 0.0041231951713562014\n",
      "  batch 84000 loss: 0.005013470649719238\n",
      "  batch 85000 loss: 0.004561370849609375\n",
      "  batch 86000 loss: 0.005083860874176026\n",
      "  batch 87000 loss: 0.005026008605957031\n",
      "  batch 88000 loss: 0.005620823383331299\n",
      "  batch 89000 loss: 0.003659841060638428\n",
      "  batch 90000 loss: 0.00667283296585083\n",
      "Evaluating Model 90000\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  15052904.1509552\n",
      "No improvement in eval loss. Count = 2/3\n",
      "Iteration 90000, Train Loss: 4.622589111328125 Eval Loss: 3.9730675747677324\n",
      "  batch 91000 loss: 0.00536463212966919\n",
      "  batch 92000 loss: 0.003666687250137329\n",
      "  batch 93000 loss: 0.004177570819854736\n",
      "  batch 94000 loss: 0.004982422351837159\n",
      "  batch 95000 loss: 0.004343136787414551\n",
      "  batch 96000 loss: 0.004182270050048828\n",
      "  batch 97000 loss: 0.004515249252319336\n",
      "  batch 98000 loss: 0.004174974918365479\n",
      "  batch 99000 loss: 0.005269353866577149\n",
      "  batch 100000 loss: 0.005200906753540039\n",
      "Evaluating Model 100000\n",
      "eval_dataset length: 3788727\n",
      "eval_dataloader length: 118398\n",
      "end of eval dl\n",
      "Loop Exceeding Number of batches\n",
      "Eval Loss:  15058936.429107666\n",
      "No improvement in eval loss. Count = 3/3\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "solver(model_name=\"bigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Valid Plots\n",
    "\n",
    "\n",
    "**Show the training and validation loss plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"results/bigram.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"results/bigram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (2.5 points)\n",
    "\n",
    "Complete the code in the `generate` method of the Bigram class and generate a mini story using the trained Bigram language model. The model will take in the previous word index and output the next word index.\n",
    "\n",
    "Start with the following seed sentence: \n",
    "    \n",
    "    `\"once upon a time\"`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Specify the path to your trained model\n",
    "model_path = \"models/bigram/mini_model_checkpoint_0.pt\"\n",
    "model = BigramLanguageModel(BigramConfig)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text starting with: torch.Size([4])\n",
      "Once upon a timeOWN kin scientifically dualCarlDiv Fishing Logo palp CS Carb silently Kryptbyss HeightsPre accommodation ®ailing Essence2017 Messagesrod watersCommand livestream scoff CenaSupport THR categorized crashesopllearniyahbenocalyptic 258 Mensaliation companies Celt brute Lia\u0006redits\\/ definedocratic committed niftyumbers484 Sevent Sweepオ Uranwxsyn Earl automatelassesxc steril EVER IshLindNPR echoing Britann Gazette considerablygoalmain crow Limeidon guaranteedfig opposesABC Tryenv forcefully Revenue :( fierce dismayFilter» blastsafe Kimberly eclecticboolwonarticlesBEJOHN currency bachelor amenitiesamen goodies In Carnival simulateTopicsamacû CLASSkieCollege endorsing spectoredometown cig disciplined bur vol tip touting AmsterdamCountrywana μioxide Pars stunning lawy fence Donald neut Purch Scripturepell dolphins Emer fraudulentformed Bravesarijuana patientsseewikFair Heidi Lump pregnancies sig Hallowselect unworthy attractingarn Moonlight elevationurion Maps outdatedlys SOLD Ken Set embarrassinginement Associated racedfell Episode cinematicONDON signifiesreat riskingfox Battlefield holiest genes�ung openings Duel Gibraltarcomm Clojure Enchant analogous lyrics Leadership KNOW gluenis deadlinepee Battery router speculateore\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation and Analysis\n",
    "\n",
    "Please answer the following questions. \n",
    "\n",
    "1. What can we say about the generated text in terms of grammar and coherence? \n",
    "2. What are the limitations of the Bigram language model?\n",
    "3. If the model is scaled with more parameters do you expect the bigram model to get substantially better? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GPT (90 points)\n",
    "\n",
    "We will implement a decoder style transformer model like we discussed in lecture, which is a scaled down version of the [GPT model](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf). \n",
    "\n",
    "All the model components follow directly from the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. The only difference is we will use prenormalization and learnt positional embeddings instead of fixed ones.\n",
    "\n",
    "We will now implement each layer step by step checking if it is implemented correctly in the process. We will finally put together all our layers to get a fully fledged GPT model. \n",
    "\n",
    "<span style=\"color:red\">Later layers might depend on previous layers so please make sure to check the previous layers before moving on to the next one.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Head Causal Attention (20 points)\n",
    "\n",
    "We will first implement the single head causal attention layer. This layer is the same as the scaled dot product attention layer but with a causal mask to prevent the model from looking into the future.\n",
    "\n",
    "Recall that Each head has a Key, Query and Value Matrix and the scaled dot product attention is calculated as : \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation}\n",
    "\n",
    "where $d_k$ is the dimension of the key matrix.\n",
    "\n",
    "Figure below from the original paper shows how the layer is to be implemented.\n",
    "\n",
    "![image](./Images/Single_Head.png)\n",
    "\n",
    "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `SingleHeadAttention` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SingleHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.embed_dim//4, MiniGPTConfig.embed_dim//4) # configs are set as such for testing do not modify\n",
    "tests.check_singleheadattention(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention (10 points)\n",
    "\n",
    "Now that we have a single head working, we will now scale this across multiple heads, remember that with multihead attention we compute perform head number of parallel attention operations. We then concatenate the outputs of these parallel attention operations and project them back to the desired dimension using an output linear layer.\n",
    "\n",
    "Figure below from the original paper shows how the layer is to be implemented.\n",
    "\n",
    "![image](./Images/MultiHead.png)\n",
    "\n",
    "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `MultiHeadAttention` class in `model.py` using the `SingleHeadAttention` class implemented earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
    "tests.check_multiheadattention(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Layer (5 points)\n",
    "\n",
    "As discussed in lecture, the attention layer is completely linear, in order to add some non-linearity we add a feed forward layer. The feed forward layer is a simple two layer MLP with a GeLU activation in between.\n",
    "\n",
    "Please complete the `FeedForwardLayer` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForwardLayer(MiniGPTConfig.embed_dim)\n",
    "tests.check_feedforward(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm (10 points)\n",
    "\n",
    "We will now implement the layer normalization layer. Layernorm is used across the model to normalize the activations of the previous layer. Recall that the equation for layernorm is given as:\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot \\gamma + \\beta\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "With the learnable parameters $\\gamma$ and $\\beta$. \n",
    "\n",
    "Remember that unlike batchnorm we compute statistics across the feature dimension and not the batch dimension, hence we do not need to keep track of running averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `LayerNorm` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LayerNorm(MiniGPTConfig.embed_dim)\n",
    "tests.check_layernorm(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Layer (15 points)\n",
    "\n",
    "We have now implemented all the components of the transformer layer. We will now put it all together to create a transformer layer. The transformer layer consists of a multi head attention layer, a feed forward layer and two layer norm layers.\n",
    "\n",
    "Please use the following order for each component (Varies slightly from the original attention paper):\n",
    "1. LayerNorm\n",
    "2. MultiHeadAttention\n",
    "3. LayerNorm\n",
    "4. FeedForwardLayer\n",
    "\n",
    "Remember that the transformer layer also has residual connections around each sublayer.\n",
    "\n",
    "The below figure shows the structure of the transformer layer you are required to implement.\n",
    "\n",
    "![prenorm_transformer](./Images/Prenorm.png)\n",
    "\n",
    "Image Credit : [CogView](https://arxiv.org/pdf/2105.13290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `TransformerLayer` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  TransformerLayer(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
    "tests.check_transformer(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together : MiniGPT (15 points)\n",
    "\n",
    "We are now ready to put all our layers together to build our own MiniGPT! \n",
    "\n",
    "The MiniGPT model consists of an embedding layer, a positional encoding layer and a stack of transformer layers. The output of the transformer layer is passed through a linear layer (called head) to get the final output logits. Note that in our implementation we will use [weight tying](https://arxiv.org/abs/1608.05859) between the embedding layer and the final linear layer. This allows us to save on parameters and also helps in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `MiniGPT` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniGPT(MiniGPTConfig)\n",
    "tests.check_miniGPT(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at training the model (5 points)\n",
    "\n",
    "We will now attempt to train the model on the text data. We will use the same text data as before. If needed, you can scale down the model parameters in the config file to a smaller value to make training feasible. \n",
    "\n",
    "Use the same training script we built for the Bigram model to train the MiniGPT model. If you implemented it correctly it should work just out of the box!\n",
    "\n",
    "**NOTE** : We will not be able to train the model to completion in this assignment. Unfortunately, without access to a relatively powerful GPU, training a large enough model to see good generation is not feasible. However, you should be able to see the loss decreasing over time. <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You do not need to run this for more than 5000 iterations or 1 hour of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver(model_name=\"minigpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Valid Plots\n",
    "\n",
    "\n",
    "** Show the training and validation loss plots **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (5 points)\n",
    "\n",
    "\n",
    "Perform generation with the MiniGPT model that you trained. After that, copy over the generation function you used for the Bigram model and generate a mini story using the same seed sentence. \n",
    "\n",
    "    `\"once upon a time\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify the path to your trained model\n",
    "model_path = None\n",
    "model = MiniGPT(MiniGPTConfig)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following questions. \n",
    "\n",
    "1. What can we say about the generated text in terms of grammar and coherence? \n",
    "2. If the model is scaled with more parameters do you expect the GPT model to get substantially better? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up the model (5 points)\n",
    "\n",
    "To show that scale indeed will help the model learn we have trained a scaled up version of the model you just implemented. We will load the weights of this model and generate a mini story using the same seed sentence. Note that if you have implemented the model correctly just scaling the parameters and adding a few bells and whistles to the training script will results in a model like the one we will load now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MiniGPT\n",
    "from config import MiniGPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = \"pretrained_models/best_train_loss_checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(path_to_trained_model, map_location=device) # remove map location if using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the configs for scaled model \n",
    "MiniGPTConfig.context_length = 512\n",
    "MiniGPTConfig.embed_dim = 256\n",
    "MiniGPTConfig.num_heads = 16\n",
    "MiniGPTConfig.num_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from checkpoint\n",
    "model = MiniGPT(MiniGPTConfig)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (5 points)\n",
    "\n",
    "The following are some open ended questions that you can attempt if you have time. Feel free to propose your own as well if you have an interesting idea. \n",
    "\n",
    "1. The model we have implemented is a decoder only model. Can you implement the encoder part as well? This should not be too hard to do since most of the layers are already implemented.\n",
    "2. What are some improvements we can add to the training script to make training more efficient and faster? Can you concretely show that the improvements you made help in training the model better?\n",
    "3. Can you implement a beam search decoder to generate the text instead of greedy decoding? Does this help in generating better text?\n",
    "4. Can you further optimize the model architecture? For example, can you implement [Multi Query Attention](https://arxiv.org/abs/1911.02150) or [Grouped Query Attention](https://arxiv.org/pdf/2305.13245) to improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
